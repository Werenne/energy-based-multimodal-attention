@ARTICLE{energy-estimation,
       author = {{Kim}, Taesup and {Bengio}, Yoshua},
        title = "{Deep Directed Generative Models with Energy-Based Probability Estimation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2016",
        month = "Jun",
          eid = {arXiv:1606.03439},
        pages = {arXiv:1606.03439},
archivePrefix = {arXiv},
       eprint = {1606.03439},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160603439K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{self-capsule,
       author = {{Hoogi}, Assaf and {Wilcox}, Brian and {Gupta}, Yachee and
         {Rubin}, Daniel L.},
        title = "{Self-Attention Capsule Networks for Image Classification}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = "2019",
        month = "Apr",
          eid = {arXiv:1904.12483},
        pages = {arXiv:1904.12483},
archivePrefix = {arXiv},
       eprint = {1904.12483},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190412483H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@incollection{gan,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {2672--2680},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
}

@ARTICLE{kingma-vae,
       author = {{Kingma}, Diederik P and {Welling}, Max},
        title = "{Auto-Encoding Variational Bayes}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2013",
        month = "Dec",
          eid = {arXiv:1312.6114},
        pages = {arXiv:1312.6114},
archivePrefix = {arXiv},
       eprint = {1312.6114},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1312.6114K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@ARTICLE{bahdanau,
       author = {{Bahdanau}, Dzmitry and {Cho}, Kyunghyun and {Bengio}, Yoshua},
        title = "{Neural Machine Translation by Jointly Learning to Align and Translate}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = "2014",
        month = "Sep",
          eid = {arXiv:1409.0473},
        pages = {arXiv:1409.0473},
archivePrefix = {arXiv},
       eprint = {1409.0473},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1409.0473B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{petersen,
author = {Petersen, Steven E. and Posner, Michael I.},
title = {The Attention System of the Human Brain: 20 Years After},
journal = {Annual Review of Neuroscience},
volume = {35},
number = {1},
pages = {73-89},
year = {2012},
doi = {10.1146/annurev-neuro-062111-150525},
    note ={PMID: 22524787},

URL = { 
        https://doi.org/10.1146/annurev-neuro-062111-150525
    
},
eprint = { 
        https://doi.org/10.1146/annurev-neuro-062111-150525
    
}
}

@article{neuro-level,
author = {Desimone, Robert and Duncan, John},
title = {Neural Mechanisms of Selective Visual Attention},
journal = {Annual Review of Neuroscience},
volume = {18},
number = {1},
pages = {193-222},
year = {1995},
doi = {10.1146/annurev.ne.18.030195.001205},
    note ={PMID: 7605061},

URL = { 
        https://doi.org/10.1146/annurev.ne.18.030195.001205
    
},
eprint = { 
        https://doi.org/10.1146/annurev.ne.18.030195.001205
    
}
}

@book{watzl,
	year = {2017},
	title = {Structuring Mind. The Nature of Attention and How It Shapes Consciousness},
	publisher = {Oxford, UK: Oxford University Press},
	author = {Sebastian Watzl}
}

@article{amplification,
    author = {Fazekas, Peter and Nanay, Bence},
    title = "{Attention Is Amplification, Not Selection}",
    journal = {The British Journal for the Philosophy of Science},
    year = {2018},
    month = {10},
    abstract = "{We argue that recent empirical findings and theoretical models shed new light on the nature of attention. According to the resulting amplification view, attentional phenomena can be unified at the neural level as the consequence of the amplification of certain input signals of attention-independent perceptual computations. This way of identifying the core realizer of attention evades standard criticisms often raised against sub-personal accounts of attention. Moreover, this approach also reframes our thinking about the function of attention by shifting the focus from the function of selection to the function of amplification.}",
    issn = {0007-0882},
    doi = {10.1093/bjps/axy065},
    url = {https://doi.org/10.1093/bjps/axy065},
    eprint = {http://oup.prod.sis.lan/bjps/advance-article-pdf/doi/10.1093/bjps/axy065/25875820/axy065.pdf},
}




@Article{manifold-3,
author = {Weinberger, K.Q. and Saul, L.K.},
title = {Unsupervised Learning of Image Manifolds by Semidefinite Programming},
journal = {Int J Comput Vision},
year = {2006}
}

@incollection{manifold-2,
title = {Sample Complexity of Testing the Manifold Hypothesis},
author = {Hariharan Narayanan and Sanjoy Mitter},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
pages = {1786--1794},
year = {2010},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3958-sample-complexity-of-testing-the-manifold-hypothesis.pdf}
}

@inproceedings{manifold-1,
  title={Algorithms for manifold learning},
  author={Lawrence Cayton},
  year={2005}
}

@ARTICLE{optim-algos,
       author = {{Ruder}, Sebastian},
        title = "{An overview of gradient descent optimization algorithms}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = "2016",
        month = "Sep",
          eid = {arXiv:1609.04747},
        pages = {arXiv:1609.04747},
archivePrefix = {arXiv},
       eprint = {1609.04747},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160904747R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{backprop,
 editor = {Chauvin, Yves and Rumelhart, David E.},
 title = {Backpropagation: Theory, Architectures, and Applications},
 year = {1995},
 isbn = {0-8058-1259-8},
 publisher = {L. Erlbaum Associates Inc.},
 address = {Hillsdale, NJ, USA}
} 

@book{goodfellow-book,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@Article{unsupervised,
author = {Z. Ghahramani},
title = {Unsupervised Learning},
journal = {Springer},
year = {2004},
OPTkey = {3176},
}


@ARTICLE{supervised,
       author = {{Loog}, Marco},
        title = "{Supervised Classification: Quite a Brief Overview}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
         year = "2017",
        month = "Oct",
          eid = {arXiv:1710.09230},
        pages = {arXiv:1710.09230},
archivePrefix = {arXiv},
       eprint = {1710.09230},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171009230L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{reinforcement,
       author = {{Li}, Yuxi},
        title = "{Deep Reinforcement Learning: An Overview}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = "2017",
        month = "Jan",
          eid = {arXiv:1701.07274},
        pages = {arXiv:1701.07274},
archivePrefix = {arXiv},
       eprint = {1701.07274},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170107274L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{attention-review,
       author = {{Galassi}, Andrea and {Lippi}, Marco and {Torroni}, Paolo},
        title = "{Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2019",
        month = "Feb",
          eid = {arXiv:1902.02181},
        pages = {arXiv:1902.02181},
archivePrefix = {arXiv},
       eprint = {1902.02181},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190202181G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{attention-need,
       author = {{Vaswani}, Ashish and {Shazeer}, Noam and {Parmar}, Niki and
         {Uszkoreit}, Jakob and {Jones}, Llion and {Gomez}, Aidan N. and
         {Kaiser}, Lukasz and {Polosukhin}, Illia},
        title = "{Attention Is All You Need}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = "2017",
        month = "Jun",
          eid = {arXiv:1706.03762},
        pages = {arXiv:1706.03762},
archivePrefix = {arXiv},
       eprint = {1706.03762},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170603762V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@InProceedings{multi-level,
author = {Yu, Dongfei and Fu, Jianlong and Mei, Tao and Rui, Yong},
title = {Multi-Level Attention Networks for Visual Question Answering},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@ARTICLE{combine-multimod,
       author = {{Liu}, Kuan and {Li}, Yanen and {Xu}, Ning and {Natarajan}, Prem},
        title = "{Learn to Combine Modalities in Multimodal Deep Learning}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = "2018",
        month = "May",
          eid = {arXiv:1805.11730},
        pages = {arXiv:1805.11730},
archivePrefix = {arXiv},
       eprint = {1805.11730},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180511730L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{anomaly-detection-energy,
       author = {{Zhai}, Shuangfei and {Cheng}, Yu and {Lu}, Weining and
         {Zhang}, Zhongfei},
        title = "{Deep Structured Energy Based Models for Anomaly Detection}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2016",
        month = "May",
          eid = {arXiv:1605.07717},
        pages = {arXiv:1605.07717},
archivePrefix = {arXiv},
       eprint = {1605.07717},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160507717Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{audiovisual-attention,
       author = {{Shon}, Suwon and {Oh}, Tae-Hyun and {Glass}, James},
        title = "{Noise-tolerant Audio-visual Online Person Verification using an Attention-based Neural Network Fusion}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Audio and Speech Processing},
         year = "2018",
        month = "Nov",
          eid = {arXiv:1811.10813},
        pages = {arXiv:1811.10813},
archivePrefix = {arXiv},
       eprint = {1811.10813},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181110813S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{latent-attention,
       author = {{Grimm}, Christopher and {Arumugam}, Dilip and {Karamcheti}, Siddharth and
         {Abel}, David and {Wong}, Lawson L.~S. and {Littman}, Michael L.},
        title = "{Modeling Latent Attention Within Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence},
         year = "2017",
        month = "Jun",
          eid = {arXiv:1706.00536},
        pages = {arXiv:1706.00536},
archivePrefix = {arXiv},
       eprint = {1706.00536},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170600536G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{stacked-attention,
       author = {{Lee}, Kuang-Huei and {Chen}, Xi and {Hua}, Gang and {Hu}, Houdong and
         {He}, Xiaodong},
        title = "{Stacked Cross Attention for Image-Text Matching}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = "2018",
        month = "Mar",
          eid = {arXiv:1803.08024},
        pages = {arXiv:1803.08024},
archivePrefix = {arXiv},
       eprint = {1803.08024},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180308024L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{crossmodal-object-detection,
       author = {{Li}, Guanbin and {Gan}, Yukang and {Wu}, Hejun and {Xiao}, Nong and
         {Lin}, Liang},
        title = "{Cross-Modal Attentional Context Learning for RGB-D Object Detection}",
      journal = {IEEE Transactions on Image Processing},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = "2019",
        month = "Apr",
       volume = {28},
       number = {4},
        pages = {1591-1601},
          doi = {10.1109/TIP.2018.2878956},
archivePrefix = {arXiv},
       eprint = {1810.12829},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ITIP...28.1591L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{crossmodal-video-caption,
       author = {{Wang}, Xin and {Wang}, Yuan-Fang and {Wang}, William Yang},
        title = "{Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
         year = "2018",
        month = "Apr",
          eid = {arXiv:1804.05448},
        pages = {arXiv:1804.05448},
archivePrefix = {arXiv},
       eprint = {1804.05448},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180405448W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{cross-transformer,
       author = {{Libovick{\'y}}, Jind{\v{r}}ich and {Helcl}, Jind{\v{r}}ich and
         {Mare{\v{c}}ek}, David},
        title = "{Input Combination Strategies for Multi-Source Transformer Decoder}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = "2018",
        month = "Nov",
          eid = {arXiv:1811.04716},
        pages = {arXiv:1811.04716},
archivePrefix = {arXiv},
       eprint = {1811.04716},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181104716L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{looking-to-listen,
       author = {{Ephrat}, Ariel and {Mosseri}, Inbar and {Lang}, Oran and {Dekel}, Tali and
         {Wilson}, Kevin and {Hassidim}, Avinatan and {Freeman}, William T. and
         {Rubinstein}, Michael},
        title = "{Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Sound, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Audio and Speech Processing},
         year = "2018",
        month = "Apr",
          eid = {arXiv:1804.03619},
        pages = {arXiv:1804.03619},
archivePrefix = {arXiv},
       eprint = {1804.03619},
 primaryClass = {cs.SD},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180403619E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{deep-lip-comparison,
       author = {{Afouras}, Triantafyllos and {Son Chung}, Joon and {Zisserman}, Andrew},
        title = "{Deep Lip Reading: a comparison of models and an online application}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = "2018",
        month = "Jun",
          eid = {arXiv:1806.06053},
        pages = {arXiv:1806.06053},
archivePrefix = {arXiv},
       eprint = {1806.06053},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180606053A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{lecun-dl,
title = "Deep learning",
abstract = "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
author = "Yann LeCun and Yoshua Bengio and Geoffrey Hinton",
year = "2015",
month = "5",
day = "27",
doi = "10.1038/nature14539",
language = "English (US)",
volume = "521",
pages = "436--444",
journal = "Nature",
issn = "0028-0836",
publisher = "Nature Publishing Group",
number = "7553",
}

@ARTICLE{lidar-camera,
       author = {{Caltagirone}, Luca and {Bellone}, Mauro and {Svensson}, Lennart and
         {Wahde}, Mattias},
        title = "{LIDAR-Camera Fusion for Road Detection Using Fully Convolutional Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = "2018",
        month = "Sep",
          eid = {arXiv:1809.07941},
        pages = {arXiv:1809.07941},
archivePrefix = {arXiv},
       eprint = {1809.07941},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180907941C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Article{lorimer2008,
author="Lorimer, R. Duncan",
title="Binary and Millisecond Pulsars",
journal="Living Reviews in Relativity",
year="2008",
month="Nov",
day="04",
volume="11",
number="1",
pages="8",
abstract="We review the main properties, demographics and applications of binary and millisecond radio pulsars. Our knowledge of these exciting objects has greatly increased in recent years, mainly due to successful surveys which have brought the known pulsar population to over 1800. There are now 83 binary and millisecond pulsars associated with the disk of our Galaxy, and a further 140 pulsars in 26 of the Galactic globular clusters. Recent highlights include the discovery of the young relativistic binary system PSR J1906+0746, a rejuvination in globular cluster pulsar research including growing numbers of pulsars with masses in excess of 1.5 M⊙, a precise measurement of relativistic spin precession in the double pulsar system and a Galactic millisecond pulsar in an eccentric (e = 0.44) orbit around an unevolved companion.",
issn="1433-8351",
doi="10.12942/lrr-2008-8",
url="https://doi.org/10.12942/lrr-2008-8"
}

@PhdThesis{lyon,
author = {R. J. Lyon},
title = {Why are pulsars hard to find},
school = {The University of Manchester},
year = {2016}
}

@Book{lorimer,
ALTauthor = {D.R Lorimer and M. Kramer},
author = {Lorimer D.R. and Kramer M.},
title = {Handbook of pulsar astronomy.},
publisher = {Cambridge Uni- versity Press},
year = {2005},
}


@Article{star-eater,
author = {J. Treat and Stegmaier},
title = {Black Holes: Star Eater.},
journal = {National Geographic},
year = {2014}
}

@INPROCEEDINGS{ebm-tutorial,
    author = {Yann LeCun and Sumit Chopra and Raia Hadsell and Fu Jie Huang and et al.},
    title = {A tutorial on energy-based learning},
    booktitle = {PREDICTING STRUCTURED DATA},
    year = {2006},
    publisher = {MIT Press}
}

@Article{ghosh,
author = {Pranab Ghosh},
title = {Rotation and Accretion Powered Pulsars
},
journal = {World Scientific Series in Astronomy and Astrophysics},
year = {2007},
OPTvolume = {10},
}


@ARTICLE{taxomany-multimodal, 
author={T. {Baltrušaitis} and C. {Ahuja} and L. {Morency}}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Multimodal Machine Learning: A Survey and Taxonomy}, 
year={2019}, 
volume={41}, 
number={2}, 
pages={423-443}, 
keywords={artificial intelligence;learning (artificial intelligence);signal processing;user interfaces;multimodal signals;specific multimodal applications;vibrant multidisciplinary field;multimodal machine learning;Speech recognition;Visualization;Media;Speech;Multimedia communication;Streaming media;Hidden Markov models;Multimodal;machine learning;introductory;survey}, 
doi={10.1109/TPAMI.2018.2798607}, 
ISSN={0162-8828}, 
month={Feb},}

@article{machine-translation,
  title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
  author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Lukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Gregory S. Corrado and Macduff Hughes and Jeffrey Dean},
  journal={ArXiv},
  year={2016},
  volume={abs/1609.08144}
}

@INPROCEEDINGS{image-recognition, 
author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Deep Residual Learning for Image Recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={770-778}, 
keywords={image classification;learning (artificial intelligence);neural nets;object detection;COCO segmentation;ImageNet localization;ILSVRC & COCO 2015 competitions;deep residual nets;COCO object detection dataset;visual recognition tasks;CIFAR-10;ILSVRC 2015 classification task;ImageNet test set;VGG nets;residual nets;ImageNet dataset;residual function learning;deeper neural network training;image recognition;deep residual learning;Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}, 
doi={10.1109/CVPR.2016.90}, 
ISSN={1063-6919}, 
month={June},}

@ARTICLE{deeplearning-overview,
       author = {{Fan}, Jianqing and {Ma}, Cong and {Zhong}, Yiqiao},
        title = "{A Selective Overview of Deep Learning}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Methodology},
         year = "2019",
        month = "Apr",
          eid = {arXiv:1904.05526},
        pages = {arXiv:1904.05526},
archivePrefix = {arXiv},
       eprint = {1904.05526},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190405526F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@ARTICLE{afouras,
       author = {{Afouras}, Triantafyllos and {Son Chung}, Joon and {Senior}, Andrew and
         {Vinyals}, Oriol and {Zisserman}, Andrew},
        title = "{Deep Audio-Visual Speech Recognition}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = "2018",
        month = "Sep",
          eid = {arXiv:1809.02108},
        pages = {arXiv:1809.02108},
archivePrefix = {arXiv},
       eprint = {1809.02108},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180902108A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{jaiswal,
       author = {{Jaiswal}, Ayush and {Sabir}, Ekraam and {AbdAlmageed}, Wael and
         {Natarajan}, Premkumar},
        title = "{Multimedia Semantic Integrity Assessment Using Joint Embedding Of Images And Text}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Multimedia, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
         year = "2017",
        month = "Jul",
          eid = {arXiv:1707.01606},
        pages = {arXiv:1707.01606},
archivePrefix = {arXiv},
       eprint = {1707.01606},
 primaryClass = {cs.MM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170701606J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{dae-vincent,
       author = {{Vincent}, Pascal and {Larochelle}, Hugo,  {Bengio}, Yoshua and and
         {Manzagol}, Pierre-Antoine},
        title = "{Extracting and Composing Robust Features with Denoising
Autoencoders}",
      journal = {ICML 2008},
     keywords = {Computer Science - Multimedia, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
         year = "2008",
}

@article{alainbengio,
       author = {{Alain}, Guillaume and {Bengio}, Yoshua},
        title = "{What Regularized Auto-Encoders Learn from the Data Generating Distribution}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2012",
        month = "Nov",
          eid = {arXiv:1211.4246},
        pages = {arXiv:1211.4246},
archivePrefix = {arXiv},
       eprint = {1211.4246},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012arXiv1211.4246A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{potentialenergy,
       author = {{Kamyshanska}, Hanna  and {Memisevic}, Roland},
        title = "{The Potential Energy of an Autoencoder}",
      journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (TPAMI)},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2014"
}

@ARTICLE{liu-kuan,
       author = {{Liu}, Kuan and {Li}, Yanen and {Xu}, Ning and {Natarajan}, Prem},
        title = "{Learn to Combine Modalities in Multimodal Deep Learning}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = "2018",
        month = "May",
          eid = {arXiv:1805.11730},
        pages = {arXiv:1805.11730},
archivePrefix = {arXiv},
       eprint = {1805.11730},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180511730L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{cocktail-party,
  author = {{Cocktail party effect}},
  title = "Cocktail party effect --- {W}ikipedia{,} The Free Encyclopedia",
  year = "2010",
  url = "https://en.wikipedia.org/wiki/Cocktail_party_effect",
  note = "[Online; accessed 29-April-2019]"
}

@ARTICLE{rbm,
       author = {{Montufar}, Guido},
        title = "{Restricted Boltzmann Machines: Introduction and Review}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Information Theory, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
         year = "2018",
        month = "Jun",
          eid = {arXiv:1806.07066},
        pages = {arXiv:1806.07066},
archivePrefix = {arXiv},
       eprint = {1806.07066},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180607066M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{crossmodal,
title = "Crossmodal attention",
journal = "Current Opinion in Neurobiology",
volume = "8",
number = "2",
pages = "245 - 253",
year = "1998",
issn = "0959-4388",
doi = "https://doi.org/10.1016/S0959-4388(98)80147-5",
url = "http://www.sciencedirect.com/science/article/pii/S0959438898801475",
author = "Jon Driver and Charles Spence",
abstract = "Most selective attention research has considered only a single sensory modality at a time, but in the real world, our attention must be coordinated crossmodally. Recent studies reveal extensive crossmodal links in attention across the various modalities (i.e. audition, vision, touch and proprioception). Attention typically shifts to a common location across the modalities, despite the vast differences in their initial coding of space. These spatial synergies in attention can be maintained even when receptors are realigned across the modalities by changes in posture. Some crossmodal integration can arise preattentively. The mechanisms underlying these crossmodal links can be examined in a convergent manner by integrating behavioural studies of normal subjects and brain-damaged patients with neuroimaging and neurophysiological studies."
}

@ARTICLE{EBM,
       author = {{Osogami}, Takayuki},
        title = "{Boltzmann machines and energy-based models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Neural and Evolutionary Computing},
         year = "2017",
        month = "Aug",
          eid = {arXiv:1708.06008},
        pages = {arXiv:1708.06008},
archivePrefix = {arXiv},
       eprint = {1708.06008},
 primaryClass = {cs.NE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170806008O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{ising-model,
  title = {The Spontaneous Magnetization of a Two-Dimensional Ising Model},
  author = {Yang, C. N.},
  journal = {Phys. Rev.},
  volume = {85},
  issue = {5},
  pages = {808--816},
  numpages = {0},
  year = {1952},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.85.808},
  url = {https://link.aps.org/doi/10.1103/PhysRev.85.808}
}

@article{integrability-criterion,
  title = {Birkhofian generalization of Hamiltionian Mechanics},
  author = {Santilli, RM},
  journal = {Foundations of theoretical mechanics II},
  year = {1982},
}

@ARTICLE{kahneman,
  
AUTHOR={Bruya, Brian and Tang, Yi-Yuan},   
	 
TITLE={Is Attention Really Effort? Revisiting Daniel Kahneman’s Influential 1973 Book Attention and Effort},      
	
JOURNAL={Frontiers in Psychology},      
	
VOLUME={9},      

PAGES={1133},     
	
YEAR={2018},      
	  
URL={https://www.frontiersin.org/article/10.3389/fpsyg.2018.01133},       
	
DOI={10.3389/fpsyg.2018.01133},      
	
ISSN={1664-1078},   
   
ABSTRACT={Daniel Kahneman was not the first to suggest that attention and effort are closely associated, but his 1973 book Attention and Effort, which claimed that attention can be identified with effort, cemented the association as a research paradigm in the cognitive sciences.  Since then, the paradigm has rarely been questioned and appears to have set the research agenda so that it is self-reinforcing.    In this article, we retrace Kahneman's argument to understand its strengths and weaknesses.  The central notion of effort is not clearly defined in the book, so we proceed by constructing the most secure inferences we can from Kahneman's argument regarding effort: it is cognitive, objective, metabolic expenditure, and it is attention.  Continuing, we find from Kahneman's argument that effort-attention must be a special case of sympathetic dominance of the autonomic nervous system that is also an increase in metabolic activity in the brain that has crossed a threshold of magnitude.  We then weigh this conception of effort against evidence in Kahneman's book and against more recent evidence, finding that it does not warrant the conclusion that effort can be equated with attention.  In support of an alternative perspective, we briefly review diverse studies of behavior, physiology and neuroscience on attention and effort, including meditation and studies of the LC-NE system, where we find evidence for the following: 1) Attention seems to be associated not with the utilization of metabolic resources per se but with the readying of metabolic resources in the form of adaptive gain modulation.  This occurs under sympathetic dominance and can be experienced as effortful.  2) Attention can also occur under parasympathetic dominance, in which case it is likely experienced as effortless.}
}

@InProceedings{pca-ae-1,
author="Scholz, Matthias
and Fraunholz, Martin
and Selbig, Joachim",
editor="Gorban, Alexander N.
and K{\'e}gl, Bal{\'a}zs
and Wunsch, Donald C.
and Zinovyev, Andrei Y.",
title="Nonlinear Principal Component Analysis: Neural Network Models and Applications",
booktitle="Principal Manifolds for Data Visualization and Dimension Reduction",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="44--67",
abstract="Nonlinear principal component analysis (NLPCA) as a nonlinear generalisation of standard principal component analysis (PCA) means to generalise the principal components from straight lines to curves. This chapter aims to provide an extensive description of the autoassociative neural network approach for NLPCA. Several network architectures will be discussed including the hierarchical, the circular, and the inverse model with special emphasis to missing data. Results are shown from applications in the field of molecular biology. This includes metabolite data analysis of a cold stress experiment in the model plant Arabidopsis thaliana and gene expression analysis of the reproductive cycle of the malaria parasite Plasmodium falciparum within infected red blood cells.",
isbn="978-3-540-73750-6"
}

@ARTICLE{pca-ae-2,
       author = {{Ladjal}, Sa{\"\i}d and {Newson}, Alasdair and {Pham}, Chi-Hieu},
        title = "{A PCA-like Autoencoder}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
         year = "2019",
        month = "Apr",
          eid = {arXiv:1904.01277},
        pages = {arXiv:1904.01277},
archivePrefix = {arXiv},
       eprint = {1904.01277},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190401277L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{kingma,
       author = {{Kingma}, Diederik P. and {Ba}, Jimmy},
        title = "{Adam: A Method for Stochastic Optimization}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = "2014",
        month = "Dec",
          eid = {arXiv:1412.6980},
        pages = {arXiv:1412.6980},
archivePrefix = {arXiv},
       eprint = {1412.6980},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{attention-is-effort,
  title={Attention and effort},
  author={Daniel Kahneman},
  year={1975}
}






