\chapter{Conclusion} 
\label{chapter-conclusion} 

The primary objective of this work was to develop a deep learning module whose task is to pre-process multi-modal inputs to reduce the amount of perturbations. In the experiments, it was indeed shown that a masking of the perturbations actually occurs. As a result, the performance of the prediction model on samples with failing modes was improved. Additionally, we experimentally verified that this pre-processing step enables the performance gain to remain stable on more intensive failing modes. In contrast, models trained only with standard data augmentation experienced a decrease of the performance on modes containing more perturbations than in the training set. Despite these promising results, caution must be taken because we were unable to investigate the performance of the module on more complex datasets. Nevertheless, we believe that the ideas detailed in Chapter \ref{chapter-emma} are sufficiently general to be considered as a starting framework for the construction of more robust multi-modal neural networks.

Another main insight was to translate the concept of capacity from psychology to deep learning. This led to the idea of a regularizer forcing the module to limit the amount of extracted information from the input. The concept of capacity and its corresponding regularizer could eventually be applied to deep learning models with a limited amount of processing power as in embedded systems. 

The last contribution proposed an architecture for a unified multi-modal attention, combining the two main types of attention mechanisms found in deep learning (i.e., self and crossmodal) with our attention module.

%----------------------------------------------------------------------------------------
%	SECTION 
%----------------------------------------------------------------------------------------

\section{Future work}
Our results are encouraging but should be validated on more complex datasets containing images, text, sounds, etc. Several points would need to be addressed such as finding efficient methods to approximate the log-likelihood of those data structures\footnote{Several alternatives were proposed in Chapter \ref{chapter-emma}}. Another point is on which level to apply the attention masks, on the raw input data or on the features extracted by the encoders?

An idea that came up during this thesis but was not tested, is based on the following observation: the transition between the first and second stage of the training can be quiet "brutal" for the MMN. Indeed, weights of the MMN at the start of the second stage are optimal for uncorrupted modes. However, these weights will have to adapt immediately to weighted inputs, as an effect of EMMA. A smoother approach to consider is inspired from the process of \textit{annealing} in metallurgy; "Annealing is a process in which a solid is first heated until all particles are randomly arranged in a liquid state, followed by a slow cooling process. At each cooling temperature enough time is spend for the solid to reach thermal equilibrium."\footnote{Explanation from \href{http://www.iue.tuwien.ac.at/phd/binder/node87.html}{here}} Applying this idea to our case, we could divide the weights of the first layer of the MMN by $\tanh(1/M)$ at the start of the second stage, and set the temperature high enough to obtain a uniform distribution of importance scores ($\alpha_i \approx 1/M,\, \forall i$). As a consequence, the effect of EMMA on the inputs of the MMN would be non-existent\footnote{This assumption is only guaranteed if the parameters $g_a$ and $b_a$ are intialized as $g_a=1$ and $b_a=0$.}, keeping the latter in its local minima. Subsequently, a cooldown schedule would be applied to the temperature leading smoothly to more pronounced attention shifts on the input data. This guided approach has no guarantee to improve the results but in our opinion can be worth trying. Moreover, instead of fixing a final temperature\footnote{Temperature at which the cooling schedule is stopped.} by tuning, a similar approach to Early Stopping could be used: the cooldown would be stopped when the validation error stops decreasing significantly.

